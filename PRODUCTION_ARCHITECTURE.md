# ğŸ—ï¸ Production Architecture - Deep Dive

**Option B: Professional Setup (GCP Cloud Run + Vercel + Cloud SQL)**

Complete system design explanation for 100-300 users with high reliability.

---

## ğŸ¯ High-Level Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         USER'S BROWSER                              â”‚
â”‚  (Chrome, Safari, Firefox - anywhere in the world)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â”‚ HTTPS (Automatic SSL)
                 â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    VERCEL EDGE NETWORK                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Next.js App (Static + Server Components)                    â”‚   â”‚
â”‚  â”‚  - HTML/CSS/JS delivered from nearest edge location          â”‚   â”‚
â”‚  â”‚  - API calls proxied to GCP                                  â”‚   â”‚
â”‚  â”‚  - Cached at 100+ locations worldwide                        â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â”‚ HTTPS (Authenticated requests)
                 â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    GCP CLOUD RUN (API SERVER)                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Express.js API (Containerized)                              â”‚   â”‚
â”‚  â”‚  - Handles uploads, queries, verification                    â”‚   â”‚
â”‚  â”‚  - Processes files (PDF extraction, chunking)                â”‚   â”‚
â”‚  â”‚  - Generates embeddings                                      â”‚   â”‚
â”‚  â”‚  - Calls OpenAI for RAG                                      â”‚   â”‚
â”‚  â”‚  - Manages authentication                                    â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                      â”‚
â”‚  Auto-scaling: 1-10 instances (based on traffic)                    â”‚
â”‚  Region: us-central1 (or closest to your users)                     â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚                                 â”‚
     â”‚                                 â”‚
     â†“                                 â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  GCP CLOUD SQL      â”‚    â”‚  SHELBY BLOCKCHAIN       â”‚
â”‚  (PostgreSQL 15)    â”‚    â”‚  (Aptos + Storage)       â”‚
â”‚                     â”‚    â”‚                          â”‚
â”‚  Stores:            â”‚    â”‚  Stores:                 â”‚
â”‚  - User accounts    â”‚    â”‚  - Actual file content   â”‚
â”‚  - Pack metadata    â”‚    â”‚  - Immutable blobs       â”‚
â”‚  - Document records â”‚    â”‚  - On-chain verification â”‚
â”‚  - Text chunks      â”‚    â”‚  - Cryptographic proofs  â”‚
â”‚  - Vector embeddingsâ”‚    â”‚                          â”‚
â”‚  - Query history    â”‚    â”‚  Access via:             â”‚
â”‚                     â”‚    â”‚  - Shelby RPC            â”‚
â”‚  Auto backups daily â”‚    â”‚  - Aptos blockchain      â”‚
â”‚  High availability  â”‚    â”‚  - Decentralized         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“Š Data Storage Strategy

### What Goes Where and Why

#### **PostgreSQL (GCP Cloud SQL)** - "The Brain"
Stores all **metadata** and **searchable data**:

```sql
-- User accounts (authentication state)
users
  â”œâ”€â”€ user_id (UUID)
  â”œâ”€â”€ email
  â””â”€â”€ created_at

-- Source packs (collections of documents)
source_packs
  â”œâ”€â”€ pack_id (UUID)
  â”œâ”€â”€ owner_user_id (â†’ users)
  â”œâ”€â”€ title, summary, tags
  â”œâ”€â”€ visibility (private/public/unlisted)
  â”œâ”€â”€ manifest_blob_id (â†’ Shelby)
  â””â”€â”€ created_at

-- Documents (file metadata)
docs
  â”œâ”€â”€ doc_id (UUID)
  â”œâ”€â”€ pack_id (â†’ source_packs)
  â”œâ”€â”€ path (filename)
  â”œâ”€â”€ mime (file type)
  â”œâ”€â”€ bytes (file size)
  â”œâ”€â”€ sha256 (content hash for verification)
  â”œâ”€â”€ shelby_blob_id (â†’ Shelby storage)
  â””â”€â”€ created_at

-- Chunks (searchable text segments)
chunks
  â”œâ”€â”€ chunk_id (UUID)
  â”œâ”€â”€ pack_id (â†’ source_packs)
  â”œâ”€â”€ doc_id (â†’ docs)
  â”œâ”€â”€ text (extracted text content)
  â”œâ”€â”€ start_byte, end_byte (position in file)
  â”œâ”€â”€ embedding (vector: float[1536])
  â””â”€â”€ created_at
```

**Size**: ~100-500MB for 100 users  
**Why PostgreSQL**: Fast queries, vector search, concurrent access

#### **Shelby Blockchain** - "The Vault"
Stores **actual file content** (immutable):

```
Shelby Storage
  â””â”€â”€ Blobs (files)
      â”œâ”€â”€ account_address/pack_id/file1.pdf
      â”œâ”€â”€ account_address/pack_id/file2.txt
      â””â”€â”€ account_address/pack_id/manifest.json

Each blob has:
  - Unique blob_id
  - SHA256 hash (on-chain)
  - Expiration date
  - Cryptographic commitment
```

**Size**: Actual files (PDFs, texts) - 10MB to 10GB  
**Why Shelby**: Verifiable, decentralized, immutable, cryptographic proofs

#### **Vercel Edge Cache** - "The CDN"
Stores **static assets** and **page cache**:

```
Vercel Edge Locations (100+ worldwide)
  â”œâ”€â”€ HTML pages (pre-rendered)
  â”œâ”€â”€ JavaScript bundles
  â”œâ”€â”€ CSS stylesheets
  â”œâ”€â”€ Images, fonts
  â””â”€â”€ API response cache (optional)
```

**Size**: ~10-50MB  
**Why Vercel**: Global CDN, instant loading anywhere

---

## ğŸ”„ Complete Data Flow (User Upload)

### **Step-by-Step: User Uploads a PDF**

```
1. USER ACTION (Browser)
   â†“
   User drags PDF file (5MB) â†’ Uploader component
   File read into memory as Buffer
   
2. FRONTEND (Next.js on Vercel)
   â†“
   FormData created with:
   - title: "Research Papers"
   - files: [paper.pdf]
   - tags: ["AI", "research"]
   
   POST request sent to:
   https://your-api.run.app/packs
   
3. VERCEL EDGE
   â†“
   Request routed to nearest edge
   Proxied to Cloud Run (with cookies)
   
4. GCP CLOUD RUN (API Server - Container)
   â†“
   a) Express receives multipart/form-data
   b) Multer extracts file from request
   c) Auth middleware validates user (cookie)
   
   d) PackManager.createPack() called
      â”œâ”€â”€ Generate pack_id (UUID)
      â”œâ”€â”€ INSERT into PostgreSQL â†’ source_packs table
      â”‚
      â”œâ”€â”€ For each file:
      â”‚   â”œâ”€â”€ Compute SHA256 hash
      â”‚   â”‚
      â”‚   â”œâ”€â”€ Upload to Shelby:
      â”‚   â”‚   â””â”€â†’ ShelbyClient.upload()
      â”‚   â”‚       â””â”€â†’ Shelby SDK
      â”‚   â”‚           â””â”€â†’ Aptos transaction (costs APT gas)
      â”‚   â”‚               â””â”€â†’ Shelby RPC stores blob
      â”‚   â”‚                   â””â”€â†’ Returns blob_id
      â”‚   â”‚
      â”‚   â”œâ”€â”€ Store in PostgreSQL:
      â”‚   â”‚   â””â”€â†’ INSERT into docs table
      â”‚   â”‚       (path, mime, bytes, sha256, shelby_blob_id)
      â”‚   â”‚
      â”‚   â”œâ”€â”€ Extract text:
      â”‚   â”‚   â””â”€â†’ TextProcessor.extractText()
      â”‚   â”‚       â””â”€â†’ pdf-parse library
      â”‚   â”‚           â””â”€â†’ Returns full text
      â”‚   â”‚
      â”‚   â”œâ”€â”€ Chunk text:
      â”‚   â”‚   â””â”€â†’ TextProcessor.chunkText()
      â”‚   â”‚       â””â”€â†’ Split into 1000-word chunks with overlap
      â”‚   â”‚
      â”‚   â””â”€â”€ Generate embeddings:
      â”‚       â””â”€â†’ For each chunk:
      â”‚           â””â”€â†’ OpenAI API call
      â”‚               â””â”€â†’ text-embedding-3-small
      â”‚                   â””â”€â†’ Returns float[1536] vector
      â”‚                       â””â”€â†’ INSERT into chunks table
      â”‚
      â””â”€â”€ Create manifest JSON
          â””â”€â†’ Upload to Shelby
              â””â”€â†’ UPDATE source_packs.manifest_blob_id
   
5. RESPONSE SENT BACK
   â†“
   Cloud Run â†’ Vercel Edge â†’ Browser
   {
     pack_id: "uuid",
     files: [
       {
         path: "paper.pdf",
         shelby_blob_id: "0x.../paper.pdf",
         sha256: "abc123...",
         indexed: true,
         chunks: 45
       }
     ]
   }
   
6. FRONTEND UPDATES
   â†“
   - Shows success message
   - Redirects to pack detail page
   - Displays file with blob_id and hash
```

**Time**: 10-30 seconds depending on file size  
**Cost per upload**: ~$0.01-0.02

---

## ğŸ” Complete Query Flow (User Asks Question)

### **Step-by-Step: "What is Shelby hot storage?"**

```
1. USER ACTION
   â†“
   Types question in chat interface
   Selects pack from dropdown
   Clicks Send
   
2. FRONTEND
   â†“
   POST https://your-api.run.app/query
   Body: {
     question: "What is Shelby hot storage?",
     pack_id: "uuid-of-pack"
   }
   Headers: { Cookie: "uid=user_id" }
   
3. CLOUD RUN API
   â†“
   a) Auth middleware validates user
   
   b) QueryEngine.queryPrivate() called:
   
      â”œâ”€â”€ Generate question embedding:
      â”‚   â””â”€â†’ OpenAI API
      â”‚       â””â”€â†’ "What is Shelby hot storage?"
      â”‚           â””â”€â†’ Returns float[1536] vector
      â”‚
      â”œâ”€â”€ Search PostgreSQL:
      â”‚   â””â”€â†’ SELECT * FROM chunks WHERE pack_id = ?
      â”‚       â””â”€â†’ Load all chunks (~1000-5000)
      â”‚           â””â”€â†’ Compute cosine similarity
      â”‚               â””â”€â†’ For each chunk:
      â”‚                   similarity = dot(query_vec, chunk_vec) / (norm * norm)
      â”‚                   â””â”€â†’ Sort by similarity
      â”‚                       â””â”€â†’ Take top 5 chunks
      â”‚
      â”œâ”€â”€ Top 5 chunks now contain:
      â”‚   â”œâ”€â”€ chunk.text (excerpt from document)
      â”‚   â”œâ”€â”€ chunk.shelby_blob_id (from docs table)
      â”‚   â”œâ”€â”€ chunk.sha256 (from docs table)
      â”‚   â””â”€â”€ chunk.score (similarity: 0.0-1.0)
      â”‚
      â”œâ”€â”€ Format context for LLM:
      â”‚   Context = "[1] " + chunk1.text + "\n\n" +
      â”‚             "[2] " + chunk2.text + ...
      â”‚
      â””â”€â”€ Call OpenAI GPT-4o-mini:
          â””â”€â†’ System: "Answer based only on provided context"
              â””â”€â†’ User: "Context:\n" + contexts + "\n\nQuestion: ..."
                  â””â”€â†’ Returns answer with [1], [2] references
   
4. RESPONSE
   â†“
   {
     answer: "Shelby hot storage refers to...[1]",
     citations: [
       {
         shelby_blob_id: "0x.../docs.pdf",
         sha256: "abc123...",
         snippet: "Hot storage in Shelby...",
         doc_path: "shelby_docs.txt",
         score: 0.89
       }
     ],
     query_time_ms: 1234
   }
   
5. FRONTEND RENDERS
   â†“
   - Shows answer with formatted text
   - Displays citation cards
   - Each citation has "Verify" button
```

**Time**: 2-5 seconds  
**Cost per query**: ~$0.002-0.005

---

## ğŸ” Authentication Flow (Dev & Production)

### **Current: Dev Mode**

```
1. User enters email
   â†“
2. POST /auth/dev-login { email }
   â†“
3. API checks PostgreSQL:
   SELECT * FROM users WHERE email = ?
   â†“
   If not found:
     - Generate UUID
     - INSERT INTO users (user_id, email)
   â†“
4. Set signed cookie:
   res.cookie('uid', user_id, {
     httpOnly: true,
     signed: true,
     maxAge: 30 days
   })
   â†“
5. All future requests include cookie
   â†“
6. Middleware validates:
   req.signedCookies.uid
   â†“
   Check user exists in database
   â†“
   Attach req.userId to request
```

### **Future: NextAuth.js (Production)**

```
1. User clicks "Sign in with Google/GitHub"
   â†“
2. OAuth flow with provider
   â†“
3. NextAuth.js session created
   â†“
4. Session stored in:
   - JWT (encrypted token) OR
   - Database session table
   â†“
5. Every request:
   import { getServerSession } from "next-auth"
   const session = await getServerSession()
   â†“
6. Forward to API with:
   x-user-id: session.user.id
```

---

## ğŸ’¾ What Data Lives Where

### **PostgreSQL Database** (Primary Data Store)

#### **users table**
```sql
Purpose: Authentication and user management
Size: ~1KB per user Ã— 300 = 300KB
Queries: On every authenticated request
```

#### **source_packs table**
```sql
Purpose: Pack metadata and organization
Size: ~500 bytes per pack
Typical: 5-10 packs per user = 1,500-3,000 packs
Total: ~750KB - 1.5MB

Contains:
- pack_id, title, summary, tags
- visibility (who can see it)
- owner_user_id (who owns it)
- manifest_blob_id (pointer to Shelby)
```

#### **docs table**
```sql
Purpose: File metadata and Shelby references
Size: ~300 bytes per document
Typical: 10 docs per pack Ã— 3000 packs = 30,000 docs
Total: ~9MB

Contains:
- Filename, mime type, size
- SHA256 hash (for verification)
- shelby_blob_id (where to find actual file)
- Relationships: which pack, which user
```

#### **chunks table** (Largest!)
```sql
Purpose: Searchable text segments with embeddings
Size: ~7KB per chunk (1KB text + 6KB vector)
Typical: 50 chunks per doc Ã— 30,000 docs = 1.5M chunks
Total: ~10GB

Contains:
- text (the actual content excerpt)
- embedding (float[1536] vector for similarity search)
- Relationships: which doc, which pack
- Byte positions (for exact citation)
```

**Total PostgreSQL Size**: ~10-15GB for 300 active users  
**Monthly Cost**: $9-15 (db-f1-micro to db-g1-small)

### **Shelby Blockchain** (Content Store)

#### **What Gets Stored**:
```
User files (PDFs, text, etc.)
  â”œâ”€â”€ Original uploaded content
  â”œâ”€â”€ Immutable once uploaded
  â”œâ”€â”€ Accessible via blob_id
  â””â”€â”€ Cryptographically verifiable

Manifests (JSON)
  â”œâ”€â”€ Pack metadata
  â”œâ”€â”€ File list with hashes
  â””â”€â”€ Used for pack reconstruction
```

**Size**: Depends on uploads
- Average: 2-5MB per pack
- 3000 packs Ã— 3MB = ~9GB
- Shelby uses erasure coding (1.6x overhead) = ~14GB actual

**Cost**: 
- Upload: APT gas + ShelbyUSD payment
- Storage: ShelbyUSD per GB per month
- Est: $30-50/month for 15GB

### **Vercel Edge** (Static Assets)

```
Next.js Build Output
  â”œâ”€â”€ _next/static/* (JS bundles)
  â”œâ”€â”€ _next/static/css/* (Stylesheets)
  â”œâ”€â”€ Prerendered HTML
  â””â”€â”€ Image optimizations

Size: ~20-50MB
Cost: $0 (included in free tier)
```

---

## ğŸ–¥ï¸ Server Architecture (GCP Cloud Run)

### **Container Specs**

```
Container: shelby-rag-api
Base Image: node:20-alpine (65MB)
Final Size: ~300MB (with dependencies)

Resources:
  CPU: 2 vCPU
  Memory: 2 GiB
  Timeout: 60 seconds
  Concurrency: 80 requests per instance
```

### **Scaling Configuration**

```yaml
Scaling:
  min_instances: 1     # Always 1 running (fast response)
  max_instances: 10    # Scale up under load
  
Auto-scaling triggers:
  - CPU > 60%
  - Request queue > 50
  - Response time > 2s
```

### **What Happens in the Container**

```
Container Startup (Cold Start):
  1. Node.js process starts
  2. Load environment variables
  3. Initialize database connection to Cloud SQL
  4. Initialize Shelby SDK with your account
  5. Initialize OpenAI client
  6. Start Express server on port 8080
  7. Ready to serve requests (2-3 seconds)

Per Request:
  1. Receive HTTP request
  2. Validate authentication (cookie/header)
  3. Process (upload/query/verify)
  4. Query PostgreSQL (~10-50ms)
  5. Call external APIs if needed:
     - Shelby upload: 1-5 seconds
     - OpenAI embed: 100-500ms
     - OpenAI LLM: 1-3 seconds
  6. Return response
  
Memory Usage:
  - Idle: ~200MB
  - Processing upload: ~500MB
  - Peak: ~1GB (large file with OCR)
```

### **Why This Works**

- **Auto-scaling**: More users â†’ more containers automatically
- **Load balancing**: GCP distributes requests evenly
- **Health checks**: Unhealthy containers replaced automatically
- **Rolling updates**: Zero-downtime deployments

---

## ğŸ”„ Request Flow (Detailed)

### **Upload Request Journey**

```
1. BROWSER
   User drags 5MB PDF
   â†“
   
2. NEXT.JS (Vercel Edge - San Francisco)
   FormData created
   Sent to: api.your-company.com/packs
   â†“ (50ms to GCP)
   
3. GCP LOAD BALANCER (us-central1)
   Receives request
   Routes to available Cloud Run instance
   â†“ (1ms)
   
4. CLOUD RUN INSTANCE #1 (Container)
   â”œâ”€â†’ Express receives request
   â”œâ”€â†’ Auth middleware: Extract cookie â†’ Query PostgreSQL
   â”‚   (5ms query)
   â”œâ”€â†’ PackManager processes:
   â”‚   â”œâ”€â†’ Compute SHA256 (10ms)
   â”‚   â”œâ”€â†’ Upload to Shelby (2-5 seconds)
   â”‚   â”‚   â””â”€â†’ Shelby SDK â†’ Aptos transaction â†’ Shelby RPC
   â”‚   â”œâ”€â†’ Extract PDF text (500ms-2s)
   â”‚   â”‚   â””â”€â†’ pdf-parse library
   â”‚   â”œâ”€â†’ Chunk text (50ms)
   â”‚   â”œâ”€â†’ Generate embeddings (2-5 seconds)
   â”‚   â”‚   â””â”€â†’ OpenAI API calls (parallel batches)
   â”‚   â””â”€â†’ Store in PostgreSQL:
   â”‚       â”œâ”€â†’ INSERT docs (5ms)
   â”‚       â””â”€â†’ INSERT chunks Ã— 45 (50ms)
   â”œâ”€â†’ Create manifest â†’ Upload to Shelby (1s)
   â””â”€â†’ Return response
   
5. RESPONSE
   â†“ (50ms to Vercel)
   
6. VERCEL EDGE
   â†“ (10ms to browser)
   
7. BROWSER
   Success! Shows pack details
```

**Total Time**: 8-15 seconds for 5MB PDF  
**Database Writes**: ~50 INSERT statements  
**External API Calls**: 2 Shelby uploads + 45 OpenAI embeds

---

## ğŸ” Security & Data Protection

### **Data at Rest**

```
PostgreSQL:
  âœ… Encrypted at rest (AES-256)
  âœ… Automated backups (retained 7 days)
  âœ… Point-in-time recovery
  âœ… SSL/TLS connections only

Shelby:
  âœ… On-chain verification
  âœ… Decentralized (no single point of failure)
  âœ… Immutable (can't be altered)
  âœ… Cryptographic proofs

Environment Variables:
  âœ… Stored in GCP Secret Manager
  âœ… Never logged
  âœ… Encrypted in transit and at rest
```

### **Data in Transit**

```
Browser â†â†’ Vercel:    HTTPS (TLS 1.3)
Vercel â†â†’ Cloud Run:  HTTPS (TLS 1.3)
Cloud Run â†â†’ SQL:     Private VPC + SSL
Cloud Run â†â†’ Shelby:  HTTPS (TLS 1.3)
Cloud Run â†â†’ OpenAI:  HTTPS (TLS 1.3)
```

### **Access Control**

```
Users can:
  âœ… Create/delete own packs
  âœ… Query own packs
  âœ… Query public packs
  âœ… Verify any citation (public endpoint)
  
Users cannot:
  âœ… Access other users' private packs
  âœ… Delete other users' packs
  âœ… Modify pack ownership
  
Rate Limits:
  - Authenticated: 100 req/min
  - Public query: 30 req/5min per IP
  - Upload: 10 files/min per user
```

---

## ğŸ’¾ Database Connection Pooling

### **How Cloud Run Connects to Cloud SQL**

```typescript
// Connection Pool Configuration
import { Pool } from 'pg'

const pool = new Pool({
  // Unix socket for Cloud SQL (more efficient than TCP)
  host: '/cloudsql/project:region:instance',
  database: 'shelby_rag',
  user: 'postgres',
  password: process.env.DB_PASSWORD,
  
  // Pool settings
  max: 5,                    // Max connections per container
  min: 1,                    // Keep 1 always warm
  idleTimeoutMillis: 30000,  // Close idle after 30s
  connectionTimeoutMillis: 2000,
})

// Reuse connections across requests
export async function query(sql, params) {
  const client = await pool.connect()
  try {
    return await client.query(sql, params)
  } finally {
    client.release()
  }
}
```

**Why This Matters**:
- Each Cloud Run instance: 5 connections
- 10 instances max: 50 total connections
- Cloud SQL can handle 100+ connections easily
- Efficient resource usage

---

## ğŸ§® Resource Calculations

### **For 300 Active Users**

#### **Concurrent Users**:
```
300 total users
Peak: ~30-50 users simultaneously
Average: ~10-20 concurrent requests
```

#### **Cloud Run Instances Needed**:
```
Each instance handles: 80 concurrent requests
For 50 concurrent: 1-2 instances sufficient
Average: 1 instance
Peak: 2-3 instances

With min_instances=1: Always ready!
```

#### **Database Connections**:
```
2 Cloud Run instances Ã— 5 connections = 10 active
Cloud SQL capacity: 100 connections (plenty of headroom)
```

#### **Request Distribution**:
```
Uploads: 10-20 per day = ~300/month
Queries: 500-1000 per day = ~20,000/month
Verifications: 100-200 per day = ~4,000/month

Total API requests: ~25,000/month
Cost: $2-5 on Cloud Run
```

---

## ğŸ”„ High Availability & Failover

### **What Happens When Things Fail**

#### **Cloud Run Instance Crashes**:
```
1. Health check fails
2. Load balancer stops routing to instance
3. New instance spins up (5-10 seconds)
4. Zero requests dropped (queued)
5. User never notices
```

#### **PostgreSQL Goes Down**:
```
1. Cloud SQL automatic failover (if HA enabled)
2. Standby promoted to primary (30-60 seconds)
3. Requests queued during failover
4. Connection pool reconnects automatically
5. Service resumes
```

#### **Shelby/OpenAI Outage**:
```
1. Uploads fail gracefully
2. Error shown to user
3. Queries to existing data still work
4. Retry mechanism in place
```

#### **Vercel Edge Failure**:
```
1. Request routed to different edge location
2. Automatic failover (milliseconds)
3. User never notices
```

### **Monitoring**

```bash
# Health checks every 10 seconds
GET /health â†’ 200 OK

# If 3 failures:
  â†’ Instance marked unhealthy
  â†’ New instance created
  â†’ Old instance drained and stopped
```

---

## ğŸ“Š Complete Tech Stack

### **Frontend Stack** (Vercel)
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Next.js 15 (App Router)    â”‚
â”‚  â”œâ”€â”€ React 18               â”‚
â”‚  â”œâ”€â”€ TypeScript             â”‚
â”‚  â”œâ”€â”€ TailwindCSS            â”‚
â”‚  â”œâ”€â”€ React Query (caching)  â”‚
â”‚  â””â”€â”€ Lucide Icons           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Build Output:
  - Server Components (streamed)
  - Client Components (hydrated)
  - Static pages (cached)
  - API routes (serverless functions)
```

### **Backend Stack** (GCP Cloud Run)
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Express.js API Server      â”‚
â”‚  â”œâ”€â”€ TypeScript (compiled)  â”‚
â”‚  â”œâ”€â”€ Multer (file uploads)  â”‚
â”‚  â”œâ”€â”€ Cookie-parser (auth)   â”‚
â”‚  â”œâ”€â”€ Rate-limit (security)  â”‚
â”‚  â””â”€â”€ CORS (configured)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Dependencies:
  - @shelby-protocol/sdk
  - @aptos-labs/ts-sdk
  - openai
  - pdf-parse
  - tesseract.js
  - better-sqlite3 â†’ pg
```

### **Database Stack** (Cloud SQL)
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PostgreSQL 15              â”‚
â”‚  â”œâ”€â”€ pgvector (future)      â”‚
â”‚  â”œâ”€â”€ Connection pooling     â”‚
â”‚  â”œâ”€â”€ Automated backups      â”‚
â”‚  â””â”€â”€ Read replicas (if HA)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Storage:
  - Data: 10-20GB
  - Backups: 30GB (7 days retention)
  - WAL logs: 5GB
```

### **External Services**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Shelby Protocol            â”‚
â”‚  â”œâ”€â”€ Aptos blockchain       â”‚
â”‚  â”œâ”€â”€ Storage providers      â”‚
â”‚  â”œâ”€â”€ RPC endpoints          â”‚
â”‚  â””â”€â”€ Indexer               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  OpenAI                     â”‚
â”‚  â”œâ”€â”€ Embeddings API         â”‚
â”‚  â”œâ”€â”€ Chat Completions API   â”‚
â”‚  â””â”€â”€ Rate limits: 500 RPM   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸŒ Network Architecture

### **Request Path with Latency**

```
User in New York
  â†“ (5ms)
Vercel Edge (New York)
  â†“ (40ms)
Cloud Run (us-central1, Iowa)
  â”œâ”€â†’ Cloud SQL (same region) - 2ms
  â”œâ”€â†’ Shelby RPC - 100ms
  â””â”€â†’ OpenAI API - 200ms
  â†“ (40ms)
Vercel Edge
  â†“ (5ms)
User

Total: ~400ms (fast!)
```

### **Geographic Distribution**

```
Vercel Edges (100+ locations):
  â”œâ”€â”€ North America (20+)
  â”œâ”€â”€ Europe (30+)
  â”œâ”€â”€ Asia (25+)
  â”œâ”€â”€ South America (10+)
  â””â”€â”€ Others (15+)

Cloud Run (1 region):
  â””â”€â”€ us-central1 (or us-west for California)

Why this works:
  - Static content served from nearest edge
  - API calls slightly slower but acceptable
  - Can add Cloud Run in multiple regions if needed
```

---

## ğŸ”§ Environment Variables by Service

### **Vercel (Frontend)**
```env
NEXT_PUBLIC_API_URL=https://api.your-domain.com

# Public variables (visible to browser)
# Keep these minimal for security
```

### **Cloud Run (Backend)**
```env
# Database
DATABASE_URL=postgresql://user:pass@/dbname?host=/cloudsql/project:region:instance

# Shelby (from Secret Manager)
SHELBY_API_KEY=AG-...
APTOS_PRIVATE_KEY=ed25519-priv-0x...
APTOS_ACCOUNT_ADDRESS=0x...

# OpenAI (from Secret Manager)
OPENAI_API_KEY=sk-proj-...

# Configuration
EMBEDDINGS_PROVIDER=openai
LLM_PROVIDER=openai
LLM_MODEL=gpt-4o-mini
CORS_ORIGIN=https://your-app.vercel.app
SESSION_SECRET=generated_secret
NODE_ENV=production
PORT=8080
```

---

## ğŸ“ˆ Performance Under Load

### **Baseline (1 user)**
- Upload: 10-30 seconds
- Query: 2-5 seconds
- Verify: 1-2 seconds

### **Peak Load (50 concurrent users)**
```
Cloud Run scales to 2-3 instances:

Instance 1: 20 requests
Instance 2: 20 requests
Instance 3: 10 requests

Each instance independent:
  - Own memory
  - Own database connections
  - Own request queue

Performance:
  - Upload: 10-40 seconds (slightly slower)
  - Query: 2-6 seconds (minimal impact)
  - Verify: 1-3 seconds
```

### **Bottlenecks**:
1. **OpenAI Rate Limits**: 500 RPM (requests per minute)
   - Solution: Request rate limiting per user
2. **Shelby Upload Speed**: Network dependent
   - Solution: Queue system for large uploads
3. **PostgreSQL Connections**: 100 max
   - Solution: Connection pooling (already implemented)

---

## ğŸ’¡ Cost Optimization Tips

### **Reduce OpenAI Costs**:
```typescript
// Cache embeddings
const cachedEmbedding = await redis.get(`emb:${textHash}`)
if (cachedEmbedding) return cachedEmbedding

// Batch embeddings
await openai.embeddings.create({
  input: texts, // Multiple at once
})
```

### **Reduce Cloud Run Costs**:
```bash
# Scale to zero when idle (demo mode)
--min-instances=0

# Use spot pricing (cheaper, but can be interrupted)
--use-spot=true
```

### **Reduce Database Costs**:
```bash
# Start small
--tier=db-f1-micro

# Upgrade only when needed
# Monitor: gcloud sql operations list
```

---

## ğŸ¯ Deployment Timeline

### **Week 1: Preparation**
- [ ] Migrate SQLite â†’ PostgreSQL (I can build adapter)
- [ ] Test with Cloud SQL locally
- [ ] Create Dockerfile
- [ ] Test Docker build

### **Week 2: Deploy & Test**
- [ ] Deploy API to Cloud Run
- [ ] Deploy Web to Vercel
- [ ] Configure secrets
- [ ] End-to-end testing

### **Week 3: Polish**
- [ ] Set up monitoring
- [ ] Configure alerts
- [ ] Load testing
- [ ] Documentation

---

## ğŸš€ Ready to Deploy?

**I can help you build:**

1. âœ… **PostgreSQL Adapter** - Replace SQLite seamlessly
2. âœ… **Docker Configuration** - Multi-stage, optimized
3. âœ… **Deployment Scripts** - One-command deploy
4. âœ… **CI/CD Pipeline** - Auto-deploy on git push
5. âœ… **Monitoring Setup** - Alerts and dashboards

**Should I start building the production infrastructure now?** 

I'll create:
- PostgreSQL database adapter
- Docker configuration for Cloud Run
- Deployment scripts
- Migration guides

This will make your deployment smooth and professional! ğŸ—ï¸

